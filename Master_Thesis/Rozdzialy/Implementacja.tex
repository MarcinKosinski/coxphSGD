\chapter{Estymacja w modelu Coxa metodą stochastycznego spadku gradientu}

Poniższy rozdział przedstawia implementację oraz zastosowanie metody stochastycznego spadku gradientu do estymacji współczynników w modelu proporcjonalnych hazardów Coxa. Jest to główny cel pracy. Poniższe rozważania odnośnie podejścia do stosowania tej metody w tym konkretnym modelu nie są oparte na żadnej literaturze ze względu na jej brak.

W czasie powstawania pracy nawiązano jedynie nieznaczną wymianę informacji z pracownikami \textit{Harvard Laboratory for Applied Statistical Methodology \& Data Science}, dzięki której dowiedziano się że podjęte zostały kroki w kierunku stworzenia podwalin pod teorię do omawianego zagadnienia, jednak ewentualne publikacje nie zostały jeszcze dokończone. Przedsmak niedokończonej implementacji algorytmu przez pracowników wyżej wymienionego laboratorium można znaleźć w \cite{sgdpkg}.

W procesie estymacji współczynników w omawianym modelu wykorzystano pochodne cząstkowe częściowej funkcji log-wiarogodności (\ref{score})
\begin{equation*}
U_k(\beta)=\dfrac{\partial\ell_k(\beta)}{\partial\beta_k}=\sum\limits_{i=1}^{K}\Big(X_{ik}-\dfrac{\sum\limits_{l\in \mathscr{R}(t_i)}^{} X_{lk} e^{X_l'\beta}}{\sum\limits_{l\in \mathscr{R}(t_i)}^{} e^{X_l'\beta}}\Big)=\sum\limits_{i=1}^{n}Y_i\Big(X_{ik}-\dfrac{\sum\limits_{l\in \mathscr{R}(t_i)}^{} X_{lk} e^{X_l'\beta}}{\sum\limits_{l\in \mathscr{R}(t_i)}^{} e^{X_l'\beta}}\Big)=\sum\limits_{i=1}^{n}U_{k_{i}}(\beta),
\end{equation*}
(dla $Y_i = 1$, gdy obserwacja nie była cenzurowana i $Y_i = 0$, gdy obserwacja była cenzurowana; $K$ to liczba zdarzeń; $n$ to liczba obserwacji) oraz wzór (\ref{sgdrownanie}), którego wersja z adekwatnymi oznaczeniami dla powyższej funkcji wygląda następująco
\begin{equation}\label{opta}
\beta_{k_{j+1}} = \beta_{k_{j}} - \alpha_{j}U_{k_{i}}(\beta_{k_{j}}),
\end{equation}
gdzie $j$ oznacza krok algorytmu, $i$ iteruje składniki $U_{k}$, $\alpha_j$ to długość $j$-tego kroku algorytmu, zaś $U_{k}$ to $k$-ta pochodna cząstkowa gradientu $U$ oraz $\beta_{k}$ to $k$-ta współrzędna estymowanego wektora współczynników $\beta$ o rozmiarze $p$, czyli $k=1,\dots,p$. 

Własna implementacja algorytmu w języku $\mathcal{R}$ znajduje się w podrozdziale (\ref{implemento}).

\section{Porównanie z innymi modelami}

Skupiając się na informatycznym aspekcie algorytmu można stwierdzić, że idea stochastycznego spadku gradientu polega na losowaniu składnika optymalizowanej funkcji. Jednak ze statystycznego punktu widzenia, metoda stochastycznego spadku gradientu opiera się o losowanie indeksu obserwacji ze zbioru, z którego uczony jest algorytm, zanim postanowi się w jakikolwiek sposób przedstawić konkretną funkcję wiarogodności. Zatem w celu estymacji w oparciu o stochastyczny spadek gradientu w modelu Coxa, konstruując metodę, należy najpierw losować obserwacje a następnie dopiero wyznaczać formę optymalizowanej funkcji częściowej log-wiarogodności.

Dla wielu modeli opierających się o funkcje wiarogodności te dwa punkty widzenia są równoważne, jednak dla modelu Coxa nie. W przypadku modelu ADALINE sformułowanego jak w \cite{ADALINE2}, opartego na minimalizowaniu funkcji kosztu w postaci błędu najmniejszych kwadratów, w \cite{bott2} podano postać funkcji straty oraz równanie algorytmu stochastycznego spadku gradientu jak poniżej
$$Q_{adaline} = \frac{1}{2}(y-w'\varPhi(x))^2,$$ 
$$w \leftarrow w + \alpha_k(y_k-w'\varPhi(x_t))\varPhi(x_t),$$
$$\varPhi(x) \in \mathbb{R}^d, y \in \{-1,1\},$$

dla których widać, że w kolejnych krokach algorytmu $t$ wystarczy tylko jedna obserwacja $z_t=(x_t,y_t)$ aby poprawić oszacowanie parametru $w$.

W modelu proporcjonalnych hazardów Coxa jest to bardziej skomplikowane.

\section{Założenia i obserwacje}\label{zalozenia}

Dla zadanej z góry funkcji hazardu, częściowa funkcja wiarogodności odpowiada prawdopodobieństwu tego, że obserwowane zdarzenia zdarzyłyby się dokładnie w tej kolejności w jakiej się pojawiły. To prawdopodobieństwo zależy od wszystkich obserwacji w zbiorze. Niemożliwe jest wyliczenie tego poprzez obliczenie wartości funkcji częściowej wiarogodności oddzielnie dla obserwacji o numerach od 1 do 5 i oddzielnie dla obserwacji od numerach 6 do 10, a następnie przemnożeniu wyników przez siebie. Z tej przyczyny niemożliwe jest losowanie czynników optymalizowanej funkcji przy użyciu metody stochastycznego spadku gradientu do estymacji współczynników w tym modelu. \textbf{Jednak możliwe jest losowanie podzbioru obserwacji a następnie konstruowanie funkcji wiarogodności dla zaobserwowanego zredukowanego zbioru.}

Taki sposób wprowadzania obserwacji do estymacji można wykorzystać w sytuacjach, gdy mamy do czynienia z nieskończonym napływem nowych obserwacji a interesują nas oszacowania estymowanych parametrów modelu $\beta_k, k=1,\dots,p$ dla obecnie zaobserwowanych i wykorzystanych obserwacji. Proces ten ma dwie zalety: nie dość, że dla nowych obserwacji model jest w stanie na bazie obecnych oszacowań parametrów dokonać predykcji proporcji hazardów, to dodatkowo po każdej porcji obserwacji aktualizuje parametry modelu.

Ponieważ
omawiane algorytmy rozwiązują problem minimalizacji badanej funkcji, zaś
celem estymacji w modelu Coxa jest znalezienie parametrów modelu
maksymalizujących funkcję częściowej log-wiarogodności, zatem wzięcie do
minimalizacji funkcji z przeciwnym znakiem doprowadzi do wykorzystania
metod znajdujących minimum do znalezienia maksimum. 

Zakładając,~że dla~\(j\)-ego kroku
algorytmu i \(k\)-tej pochodnej cząstkowej dysponuje się zaobserwowanym podzbiorem
\(\mathcal{B}\), częściową funkcję wiarogodności dla zaobserwowanego podzbioru obserwacji \(\mathcal{B}\) wykorzystywaną do minimalizacji można zapisać następująco
\begin{equation}
-U^\mathcal{B}_k(\beta_j)=-\sum\limits_{i \in \mathcal{B}_\text{ind}}^{}U^\mathcal{B}_{k_{i}}(\beta_j)=-\sum\limits_{i \in \mathcal{B}_\text{ind}}^{}Y_i\Big(X_{ik}-\dfrac{\sum\limits_{l\in \mathscr{R}_\mathcal{B}(t_i)}^{} X_{lk} e^{X_l'\beta_j}}{\sum\limits_{l\in \mathscr{R}_\mathcal{B}(t_i)}^{} e^{X_l'\beta_j}}\Big),
\end{equation}
gdzie indeksy obserwacji należące do zbioru \(\mathcal{B}\) definiuje się jako
\(\mathcal{B}_{\text{ind}} = \{i: X_i \in \mathcal{B} \}\), zaś
\(\mathscr{R}_\mathcal{B}(t_i)\) to zbiór ryzyka dla podzbioru
\(\mathcal{B}\) w czasie \(t_i\).

Postać powyższej funkcji nasuwa pewne obserwacje. Dla danego kroku algorytmu, obecnie wykorzystywany zaobserwowany podzbiór obserwacji \(\mathcal{B}\)
\begin{itemize}
\item \textit{nie powinien składać się jedynie z obserwacji cenzurowanych}. \newline \ \newline Dla podzbioru zawierającego jedynie takie obserwacje wartość pochodnej cząstkowej funkcji log-wiarogodności jest równa zero, ze względu na czynniki $Y_i$, które dla obserwacji cenzurowanych są równe zero. Zerowa wartość pochodnej cząstkowej funkcji log-wiarogodności doprowadzi do niezmienienia się optymalizowanych parametrów we wzorze (\ref{opta}), a co za tym idzie, doprowadzi do przerwania optymalizacji.
\item \textit{nie powinien zawierać obserwacji cenzurowanych, które wszystkie mają czasy obserwacji krótsze niż najmniejszy czas obserwacji dla obserwacji niecenzurowanej}. \newline \ \newline
Obserwacje cenzurowane niosą ze sobą mniej informacji, jednak teoria analizy przeżycia stara się je maksymalnie wykorzystać, stąd uwzględnia się ich udział w zbiorze ryzyka przy estymacji fragmentów funkcji log-wiarogodności dla obserwacji niecenzurowanych. W sytuacji, gdy wszystkie obserwacje cenzurowane mają czas obserwacji krótszy niż najmniejszy czas obserwacji dla obserwacji niecenzurowanej w podzbiorze, nie dochodzi do wykorzystania obserwacji cenzurowanych w jakikolwiek sposób przy estymacji współczynników w danym kroku algorytmu.
\item \textit{nie powinien składać się jedynie z jednej obserwacji}. \newline \ \newline 
W takim przypadku wartość pochodnej cząstkowej funkcji log-wiarogodności jest również równa zero, bez względu na to czy obserwacja była cenzurowana czy nie. Zerowa wartość pochodnej cząstkowej funkcji log-wiarogodności doprowadzi do niezmienienia się optymalizowanych parametrów we wzorze (\ref{opta}), a co za tym idzie, doprowadzi do przerwania optymalizacji.
\item \textit{nie powienien zawierać tylko jednej obserwacji niecenzurowanej w zbiorze, która dodatkowo ma największy czas bycia pod obserwacją}. \newline \ \newline
W takim wypadku jedyny niezerowy czynnik w całej pochodnej cząstkowej funkcji log-wiarogodności zajdzie tylko dla obserwacji niecenzurowanej, dla której zbiór ryzyka będzie zawierał tylko nią, co da ostatecznie zerową wartość pochodnej cząstkowej optymalizowanej funkcji log-wiarogodności, co doprowadzi do przerwania optymalizacji.
\end{itemize}
\newpage
\section{Implementacja}\label{implemento}

Omówiona w tym rozdziale metoda estymacji metodą stochastycznego spadku gradientu dla modelu proporcjonalnych hazardów Coxa została zaimplementowana w języku $\mathcal{R}$ \cite{programikr} i jest dostępna w specjalnie przygotowanym pakiecie o nazwie \texttt{coxphSGD}, który można pobrać z internetu i zainstalować poleceniem
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"MarcinKosinski/Cox-SGD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Dokumentacja wraz z opisem argumentów w języku angielskim funkcji \texttt{coxphSGD()}, która estymuje współczynniki w modelu
proporcjonalnych hazardów Coxa metodą stochastycznego spadku gradientu, dostępna jest w Dodatku \ref{docCoxSGD}.
Starano zachować jednorodność kolejności i nazewnictwa parametrów z funkcją \texttt{coxph} z pakietu \texttt{survival} \cite{ther}, \cite{survival}.

Implementacja algorytmu estymacji w modelu Coxa metodą stochastycznego spadku gradientu opiera się na poniższym pseudo-kodzie i zakłada, że kolejne podzbiory \(\mathcal{B}\) dostarczane są jako kolejne elementy listy.
\subsubsection{Estymacja~w~modelu~Coxa~metodą~stochastycznego~spadku~gradientu}


\begin{Shaded}
\begin{Highlighting}[]
                    \CommentTok{# wstępna inicjalizacja parametrów}
\NormalTok{eps =}\StringTok{ }\FloatTok{1e-5}                               \CommentTok{# warunek stopu.}

\NormalTok{n =}\StringTok{ }\KeywordTok{length}\NormalTok{(data)                         }\CommentTok{# data jest listą ramek danych.}

\NormalTok{diff =}\StringTok{ }\NormalTok{eps +}\StringTok{ }\DecValTok{1}                           \CommentTok{# różnice w oszacowaniach parametrów}
                                         \CommentTok{# między kolejnymi krokami.}

\NormalTok{learningRates =}\StringTok{ }\NormalTok{function(x) }\DecValTok{1}\NormalTok{/x          }\CommentTok{# długości kroku algorytmu.}

\NormalTok{beta_old =}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{length =} \NormalTok{k)        }\CommentTok{# punkt startowy dlugosci k,}
                                         \CommentTok{# gdzie k to liczba zmiennych}
                                         \CommentTok{# objaśniających w modelu.}

                              \CommentTok{# estymacja}
\NormalTok{i =}\StringTok{ }\DecValTok{1}                                    \CommentTok{# iterator kroku algorytmu}
\NormalTok{while(i <=}\StringTok{ }\NormalTok{n \&}\StringTok{ }\NormalTok{diff >}\StringTok{ }\NormalTok{eps) do            }\CommentTok{# do zbieżności lub wyczerpania zbiorów}
  \NormalTok{batch =}\StringTok{ }\NormalTok{data[[i]]}
  
  \NormalTok{beta_new =}\StringTok{ }\NormalTok{beta_old -}\StringTok{ }\KeywordTok{learningRates}\NormalTok{(i) *}\StringTok{ }\KeywordTok{U_Batch}\NormalTok{(batch) }
                                         \CommentTok{# U_Batch to częściowa funkcja}
                                         \CommentTok{# log-wiarogdności dla zaobserwowanego}
                                         \CommentTok{# zbioru `batch`}
  
  \NormalTok{diff =}\StringTok{ }\KeywordTok{euclidean_dist}\NormalTok{(beta_new, beta_old) }\CommentTok{# odległość euklidesowa}
  
  \NormalTok{beta_old =}\StringTok{ }\NormalTok{beta_new }
  
  \NormalTok{i =}\StringTok{ }\NormalTok{i +}\StringTok{ }\DecValTok{1}
\NormalTok{end while}
\NormalTok{return beta_new}
\end{Highlighting}
\end{Shaded}

\textbf{Docelowa implementacja w języku $\mathcal{R}$ znajduje się poniżej.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coxphSGD <-}\StringTok{ }\NormalTok{function(formula, data, }
                    \DataTypeTok{learningRates =} \NormalTok{function(x)\{}\DecValTok{1}\NormalTok{/x\},}
                    \DataTypeTok{beta_0 =} \DecValTok{0}\NormalTok{, }\DataTypeTok{epsilon =} \FloatTok{1e-5} \NormalTok{) \{}
  \KeywordTok{checkArguments}\NormalTok{(formula, data, learningRates,}
                  \NormalTok{beta_0, epsilon) ->}\StringTok{ }\NormalTok{beta_old }\CommentTok{# check arguments}

  \NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(data)}
  \NormalTok{diff <-}\StringTok{ }\NormalTok{epsilon +}\StringTok{ }\DecValTok{1}
  \NormalTok{i <-}\StringTok{ }\DecValTok{1}
  \NormalTok{beta_new <-}\StringTok{ }\KeywordTok{list}\NormalTok{() }\CommentTok{# steps are saved in a list so that they can}
                     \CommentTok{# be tracked in the future}
  \CommentTok{# estimate}
  \NormalTok{while(i <=}\StringTok{ }\NormalTok{n &}\StringTok{ }\NormalTok{diff >}\StringTok{ }\NormalTok{epsilon) \{}
    \CommentTok{#tryCatch(\{}
    \NormalTok{beta_new[[i]] <-}\StringTok{ }\KeywordTok{coxphSGD_batch}\NormalTok{(}\DataTypeTok{formula =} \NormalTok{formula, }\DataTypeTok{data =} \NormalTok{data[[i]],}
                                    \DataTypeTok{learningRate =} \KeywordTok{learningRates}\NormalTok{(i),}
                                    \DataTypeTok{beta =} \NormalTok{beta_old)}
    
    \NormalTok{diff <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((beta_new[[i]] -}\StringTok{ }\NormalTok{beta_old)^}\DecValTok{2}\NormalTok{))}
    \NormalTok{beta_old <-}\StringTok{ }\NormalTok{beta_new[[i]]}
    \NormalTok{i <-}\StringTok{ }\NormalTok{i +}\StringTok{ }\DecValTok{1}  
    \CommentTok{#\}, error = function(cond) \{i <<- n + 1\})}
  \NormalTok{\}}
  
  \CommentTok{# return results}
  \NormalTok{fit <-}\StringTok{ }\KeywordTok{list}\NormalTok{()}
  \NormalTok{fit$Call <-}\StringTok{ }\KeywordTok{match.call}\NormalTok{()}
  \NormalTok{fit$coefficients <-}\StringTok{ }\NormalTok{beta_new}
  \NormalTok{fit$epsilon <-}\StringTok{ }\NormalTok{epsilon}
  \NormalTok{fit$learningRates <-}\StringTok{ }\NormalTok{learningRates}
  \NormalTok{fit$steps <-}\StringTok{ }\NormalTok{i}
  \KeywordTok{class}\NormalTok{(fit) <-}\StringTok{ "coxphSGD"}
  \NormalTok{fit}
\NormalTok{\}}

\NormalTok{coxphSGD_batch <-}\StringTok{ }\NormalTok{function(formula, data, learningRate, beta)\{}
  
  \CommentTok{# Parameter identification as in  `survival::coxph()`.}
  \NormalTok{Call <-}\StringTok{ }\KeywordTok{match.call}\NormalTok{()}
  \NormalTok{indx <-}\StringTok{ }\KeywordTok{match}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"formula"}\NormalTok{, }\StringTok{"data"}\NormalTok{),}
                \KeywordTok{names}\NormalTok{(Call), }\DataTypeTok{nomatch =} \DecValTok{0}\NormalTok{)}
  \NormalTok{if (indx[}\DecValTok{1}\NormalTok{] ==}\StringTok{ }\DecValTok{0}\NormalTok{) }
      \KeywordTok{stop}\NormalTok{(}\StringTok{"A formula argument is required"}\NormalTok{)}
  \NormalTok{temp <-}\StringTok{ }\NormalTok{Call[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, indx)]}
  \NormalTok{temp[[}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.name}\NormalTok{(}\StringTok{"model.frame"}\NormalTok{)}
  
  \NormalTok{mf <-}\StringTok{ }\KeywordTok{eval}\NormalTok{(temp, }\KeywordTok{parent.frame}\NormalTok{())}
  \NormalTok{Y <-}\StringTok{ }\KeywordTok{model.extract}\NormalTok{(mf, }\StringTok{"response"}\NormalTok{)}
  
  \NormalTok{if (!}\KeywordTok{inherits}\NormalTok{(Y, }\StringTok{"Surv"}\NormalTok{)) }
      \KeywordTok{stop}\NormalTok{(}\StringTok{"Response must be a survival object"}\NormalTok{)}
  \NormalTok{type <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(Y, }\StringTok{"type"}\NormalTok{)}
  
  \NormalTok{if (type !=}\StringTok{ "right"} \NormalTok{&&}\StringTok{ }\NormalTok{type !=}\StringTok{ "counting"}\NormalTok{) }
      \KeywordTok{stop}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"Cox model doesn't support }\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, type, }\StringTok{"}\CharTok{\textbackslash{}"}\StringTok{ survival data"}\NormalTok{, }
          \DataTypeTok{sep =} \StringTok{""}\NormalTok{))}
  
  \CommentTok{# collect times, status, variables and reorder samples }
  \CommentTok{# to make the algorithm more clear to read and track}
  \KeywordTok{cbind}\NormalTok{(}\DataTypeTok{not_censored =} \DecValTok{1} \NormalTok{-}\StringTok{ }\KeywordTok{unclass}\NormalTok{(Y)[,}\DecValTok{2}\NormalTok{],}
        \DataTypeTok{times =} \KeywordTok{unclass}\NormalTok{(Y)[,}\DecValTok{1}\NormalTok{],}
        \NormalTok{mf[, -}\DecValTok{1}\NormalTok{]) %>%}
\StringTok{    }\KeywordTok{arrange}\NormalTok{(times) ->}\StringTok{ }\NormalTok{batchData}
  
  \CommentTok{# calculate the log-likelihood for this batch sample}
  \NormalTok{partial_sum <-}\StringTok{ }\KeywordTok{list}\NormalTok{()}
  
  \NormalTok{for(k in }\DecValTok{1}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(batchData)) \{}
    
    \CommentTok{# risk set for current time/observation}
    \NormalTok{risk_set <-}\StringTok{ }\NormalTok{batchData %>%}
\StringTok{      }\KeywordTok{filter}\NormalTok{(times <=}\StringTok{ }\NormalTok{batchData$times[k])}
    
    \NormalTok{nominator <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(risk_set[, -}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)], }\DataTypeTok{MARGIN =} \DecValTok{1}\NormalTok{, function(element)\{}
      \NormalTok{element *}\StringTok{ }\KeywordTok{exp}\NormalTok{(element *}\StringTok{ }\NormalTok{beta)}
    \NormalTok{\}) %>%}
\StringTok{      }\NormalTok{t %>%}
\StringTok{      }\KeywordTok{colSums}\NormalTok{()}
    
    \NormalTok{denominator <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(risk_set[, -}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)], }\DataTypeTok{MARGIN =} \DecValTok{1}\NormalTok{, function(element)\{}
      \KeywordTok{exp}\NormalTok{(element *}\StringTok{ }\NormalTok{beta)}
    \NormalTok{\}) %>%}
\StringTok{      }\NormalTok{t %>%}
\StringTok{      }\KeywordTok{colSums}\NormalTok{()}
    
    \NormalTok{partial_sum[[k]] <-}\StringTok{ }
\StringTok{      }\NormalTok{batchData[k, }\StringTok{"not_censored"}\NormalTok{] *}\StringTok{ }\NormalTok{(batchData[k, -}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)] -}\StringTok{ }\NormalTok{nominator/denominator)}
    
  \NormalTok{\}}
  
  \KeywordTok{do.call}\NormalTok{(rbind, partial_sum) %>%}
\StringTok{    }\KeywordTok{colSums}\NormalTok{() ->}\StringTok{ }\NormalTok{U_batch}
  
  \NormalTok{beta_out <-}\StringTok{ }\NormalTok{beta +}\StringTok{ }\NormalTok{learningRate *}\StringTok{ }\NormalTok{U_batch}
  
  \KeywordTok{return}\NormalTok{(beta_out)}
\NormalTok{\}}
  
\NormalTok{checkArguments <-}\StringTok{ }\NormalTok{function(formula, data, learningRates,}
                             \NormalTok{beta_0, epsilon) \{}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.list}\NormalTok{(data) &}\StringTok{ }\KeywordTok{length}\NormalTok{(data) >}\StringTok{ }\DecValTok{0}\NormalTok{)}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(data, ncol)))) ==}\StringTok{ }\DecValTok{1}\NormalTok{)}
  \CommentTok{# + check names and types for every variables}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.function}\NormalTok{(learningRates))}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(epsilon))}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(beta_0))}
  
    \CommentTok{# check length of the start parameter}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(beta_0) ==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
    \NormalTok{beta_0 <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(beta_0, }\KeywordTok{as.character}\NormalTok{(formula)[}\DecValTok{3}\NormalTok{] %>%}
\StringTok{                    }\KeywordTok{strsplit}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{+"}\NormalTok{) %>%}
\StringTok{                    }\NormalTok{unlist %>%}
\StringTok{                    }\NormalTok{length)}
  \NormalTok{\}}
  \KeywordTok{return}\NormalTok{(beta_0)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
