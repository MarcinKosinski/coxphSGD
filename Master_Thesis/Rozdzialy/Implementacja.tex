\chapter{Estymacja w modelu Coxa metodą stochastycznego spadku gradientu}

Poniższy rozdział przedstawia implementację oraz zastosowanie metody stochastycznego spadku gradientu do estymacji współczynników w modelu proporcjonalnych hazardów Coxa. Jest to główny cel pracy. Poniższe rozważania odnośnie podejścia do stosowania tej metody w tym konkretnym modelu nie są oparte na żadnej literaturze ze względu na jej brak.

W czasie powstawania pracy nawiązano jedynie nieznaczną wymianę informacji z pracownikami \textit{Harvard Laboratory for Applied Statistical Methodology \& Data Science}, dzięki której dowiedziano się że podjęte zostały kroki w kierunku stworzenia podwalin pod teorię do omawianego zagadnienia, jednak ewentualne publikacje nie zostały jeszcze dokończone. Przedsmak niedokończonej implementacji algorytmu przez pracowników wyżej wymienionego laboratorium można znaleźć w \cite{sgdpkg}.

W procesie estymacji współczynników w omawianym modelu wykorzystano pochodne cząstkowe częściowej funkcji log-wiarogodności (\ref{score})
\begin{equation*}
U_k(\beta)=\dfrac{\partial\ell_k(\beta)}{\partial\beta_k}=\sum\limits_{i=1}^{K}\Big(X_{ik}-\dfrac{\sum\limits_{l\in \mathscr{R}(t_i)}^{} X_{lk} e^{X_l'\beta}}{\sum\limits_{l\in \mathscr{R}(t_i)}^{} e^{X_l'\beta}}\Big)=\sum\limits_{i=1}^{n}Y_i\Big(X_{ik}-\dfrac{\sum\limits_{l\in \mathscr{R}(t_i)}^{} X_{lk} e^{X_l'\beta}}{\sum\limits_{l\in \mathscr{R}(t_i)}^{} e^{X_l'\beta}}\Big)=\sum\limits_{i=1}^{n}U_{k_{i}}(\beta),
\end{equation*}
(dla $Y_i = 1$, gdy obserwacja nie była cenzurowana i $Y_i = 0$, gdy obserwacja była cenzurowana; $K$ to liczba zdarzeń; $n$ to liczba obserwacji) oraz wzór (\ref{sgdrownanie}), którego wersja z adekwatnymi oznaczeniami dla powyższej funkcji wygląda następująco
\begin{equation}
\beta_{k_{j+1}} = \beta_{k_{j}} - \alpha_{j}U_{k_{i}}(\beta_{k_{j}}),
\end{equation}
gdzie $j$ oznacza krok algorytmu, $i$ iteruje składniki $U_{k}$, $\alpha_j$ to długość $j$-tego kroku algorytmu, zaś $U_{k}$ to $k$-ta pochodna cząstkowa gradientu $U$ oraz $\beta_{k}$ to $k$-ta współrzędna estymowanego wektora współczynników $\beta$ o rozmiarze $p$, czyli $k=1,\dots,p$. 

Własna implementacja algorytmu w języku $\mathcal{R}$ znajduje się w Dodatku \ref{coxKody} albo podrozdziale \ref{?}.

\section{Porównanie z innymi modelami}

Skupiając się na informatycznym aspekcie algorytmu można stwierdzić, że idea stochastycznego spadku gradientu polega na losowaniu składnika optymalizowanej funkcji. Jednak ze statystycznego punktu widzenia, metoda stochastycznego spadku gradientu opiera się o losowanie indeksu obserwacji ze zbioru, z którego uczony jest algorytm, zanim postanowi się w jakikolwiek sposób przedstawić konkretną funkcję wiarogodności. Zatem w celu estymacji w oparciu o stochastyczny spadek gradientu w modelu Coxa, konstruując metodę, należy najpierw losować obserwacje a następnie dopiero wyznaczać formę optymalizowanej funkcji częściowej log-wiarogodności.

Dla wielu modeli opierających się o funkcje wiarogodności te dwa punkty widzenia są równoważne, jednak dla modelu Coxa nie. W przypadku modelu ADALINE sformułowanego jak w \cite{ADALINE2}, opartego na minimalizowaniu funkcji kosztu w postaci błędu najmniejszych kwadratów, w \cite{bott2} podano postać funkcji straty oraz równanie algorytmu stochastycznego spadku gradientu jak poniżej
$$Q_{adaline} = \frac{1}{2}(y-w'\varPhi(x))^2,$$ 
$$w \leftarrow w + \alpha_k(y_k-w'\varPhi(x_t))\varPhi(x_t),$$
$$\varPhi(x) \in \mathbb{R}^d, y \in \{-1,1\},$$

dla których widać, że w kolejnych krokach algorytmu $t$ wystarczy tylko jedna obserwacja $z_t=(x_t,y_t)$ aby poprawić oszacowanie parametru $w$.

W modelu proporcjonalnych hazardów Coxa jest to bardziej skomplikowane.

\section{Implementacja}

Dla zadanej z góry funkcji hazardu, częściowa funkcja wiarogodności odpowiada prawdopodobieństwu tego, że obserwowane zdarzenia zdarzyłyby się dokładnie w tej kolejności w jakiej się pojawiły. To prawdopodobieństwo zależy od wszystkich obserwacji w zbiorze. Niemożliwe jest wyliczenie tego poprzez obliczenie wartości funkcji częściowej wiarogodności oddzielnie dla obserwacji o numerach od 1 do 5 i oddzielnie dla obserwacji od numerach 6 do 10, a następnie przemnożeniu wyników przez siebie. Z tej przyczyny niemożliwe jest losowanie czynników optymalizowanej funkcji przy użyciu metody stochastycznego spadku gradientu do estymacji współczynników w tym modelu. Jednak możliwe jest losowanie podzbioru obserwacji a następnie konstruowanie funkcji wiarogodności dla zaobserwowanego zredukowanego zbioru.

Taki sposób wprowadzania obserwacji do estymacji można wykorzystać w sytuacjach, gdy mamy do czynienia z nieskończonym napływem nowych obserwacji a interesują nas oszacowania estymowanych parametrów modelu $\beta_k, k=1,\dots,p$ dla obecnie zaobserwowanych i wykorzystanych obserwacji. Proces ten ma dwie zalety: nie dość, że dla nowych obserwacji model jest w stanie na bazie obecnych oszacowań parametrów dokonać predykcji proporcji hazardów, to dodatkowo po każdej porcji obserwacji aktualizuje parametry modelu.

Omówiona w tym rozdziale metoda estymacji metodą stochastycznego spadku gradientu dla modelu proporcjonalnych hazardów Coxa została zaimplementowana w języku $\mathcal{R}$ \cite{programikr}. Kod wraz z dokumentacją stworzonej funkcji w języku angielskim oraz opisem poszczególnych kroków algorytmu zawarty jest poniżej.

\newpage
Algorytm jest dostępny w specjalnie przygotowanym pakiecie o nazwie
\texttt{coxphSGD} napisanym w języku \(\mathcal{R}\), który można pobrać
z internetu i zainstalować poleceniem

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (}\KeywordTok{packageVersion}\NormalTok{(}\StringTok{"devtools"}\NormalTok{) <}\StringTok{ }\FloatTok{1.6}\NormalTok{) \{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{\}}
\NormalTok{devtools::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"MarcinKosinski/Cox-SGD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Poniżej przedstawione są argumenty, które przyjmuje funkcja
\texttt{coxphSGD()}, która estymuje współczynniki w modelu
proporcjonalnych hazardów Coxa metodą stochastycznego spadku gradientu.
Starano zachować jednorodność kolejności i nazewnictwa parametrów z
funkcją \texttt{coxph} z pakietu \texttt{survival} \cite{ther},
\cite{survival}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Stochastic Gradient Descent log-likelihood estimation in }
\CommentTok{#' Cox proportional hazards model}
\CommentTok{#' }
\CommentTok{#' Function \textbackslash{}code\{coxphSGD\} estimates coefficients using stochastic}
\CommentTok{#' gradient descent algorithm in Cox proportional hazards model.}
\CommentTok{#' }
\CommentTok{#' @param formula a formula object, with the response on the left of a ~ operator,}
\CommentTok{#' and the terms on the right. The response must be a survival object as returned by}
\CommentTok{#' the Surv function.}
\CommentTok{#' @param data a data.frame in which to interpret the variables named in the \textbackslash{}code\{formula\}.}
\CommentTok{#' @param reorderObs a logical value telling whether reorder observations at each epoch.}
\CommentTok{#' when order of observations in estimation should be randomly generated.}
\CommentTok{#' @param learningRates a function specifing how to define learning rates in }
\CommentTok{#' steps of the algorithm. By default the \textbackslash{}code\{f(t)=1/t\} is used, where \textbackslash{}code\{t\} is}
\CommentTok{#' the number of algorithm's step.}
\CommentTok{#' @param beta_0 a numeric vector (if of length 1 then will be replicated) of length }
\CommentTok{#' equal to the number of variables after using \textbackslash{}code\{formula\} in the \textbackslash{}code\{model.matrix\}}
\CommentTok{#' function}
\CommentTok{#' @param epsilon a numeric value with the stop condition of the estimation algorithm. }
\CommentTok{#' @param epoch a numeric value declaring the number of epoches to run for the}
\CommentTok{#' estimation algorithm in the stochastic gradient descent.}
\CommentTok{#' @param batchSize a numeric value specifing the size of a batch set to take from }
\CommentTok{#' the reordered dataset to update the coefficients in one step of an algorithm.}
\CommentTok{#'}
\CommentTok{#' @note If one of the conditions is fullfiled}
\CommentTok{#' \textbackslash{}itemize\{}
\CommentTok{#'  \textbackslash{}item \textbackslash{}eqn\{||\textbackslash{}beta_\{j+1\}-\textbackslash{}beta_\{j\}|| <\}\textbackslash{}code\{epsilon\} parameter for any \textbackslash{}eqn\{j\}}
\CommentTok{#'  \textbackslash{}item \textbackslash{}eqn\{#epochs >\} \textbackslash{}code\{epochs\} parameter}
\CommentTok{#' \}}
\CommentTok{#' the estimation process is stopped.}
\CommentTok{#' @export}
\CommentTok{#' @importFrom survival Surv}
\CommentTok{#' @importFrom assertthat assert_that}
\CommentTok{#' @examples}
\CommentTok{#' library(survival)}
\CommentTok{#' \textbackslash{}dontrun\{}
\CommentTok{#' coxphSGD(Surv(time, status) ~ ph.ecog + age, data=lung)}
\CommentTok{#' \}}
\CommentTok{#' }
\end{Highlighting}
\end{Shaded}

Sprawdzenie parametrów na wejściu funkcji.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coxphSGD =}\StringTok{ }\NormalTok{function(formula, data, }\DataTypeTok{reorderObs =} \OtherTok{TRUE}\NormalTok{,}
                    \DataTypeTok{learningRates =} \NormalTok{function(x) }\DecValTok{1}\NormalTok{/x,}
                    \DataTypeTok{beta_0 =} \DecValTok{0}\NormalTok{, }\DataTypeTok{epsilon =} \FloatTok{1e-5}\NormalTok{,}
                    \DataTypeTok{batchSize =} \DecValTok{10}\NormalTok{, }\DataTypeTok{epoch =} \DecValTok{20} \NormalTok{) \{}
  
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.data.frame}\NormalTok{(data))}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.logical}\NormalTok{(reorderObs))}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.function}\NormalTok{(learningRates))}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(epsilon))}
  \KeywordTok{assert_that}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(epoch) &}\StringTok{ }\NormalTok{epoch >}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Identyfikacja przekazanych parametrów. Poniższa identyfikacja bazuje na
kodzie funkcji \texttt{coxph()}.

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{Call <-}\StringTok{ }\KeywordTok{match.call}\NormalTok{()}
  \NormalTok{indx <-}\StringTok{ }\KeywordTok{match}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"formula"}\NormalTok{, }\StringTok{"data"}\NormalTok{, }\StringTok{"order"}\NormalTok{, }\StringTok{"learningRates"}\NormalTok{,}
                  \StringTok{"epsilon"}\NormalTok{, }\StringTok{"batchsize"}\NormalTok{, }\StringTok{"epoch"}\NormalTok{),}
                \KeywordTok{names}\NormalTok{(Call), }\DataTypeTok{nomatch =} \DecValTok{0}\NormalTok{)}
  \NormalTok{if (indx[}\DecValTok{1}\NormalTok{] ==}\StringTok{ }\DecValTok{0}\NormalTok{) }
      \KeywordTok{stop}\NormalTok{(}\StringTok{"A formula argument is required"}\NormalTok{)}
  \NormalTok{temp <-}\StringTok{ }\NormalTok{Call[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, indx)]}
  \NormalTok{temp[[}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\KeywordTok{as.name}\NormalTok{(}\StringTok{"model.frame"}\NormalTok{)}
  
  \NormalTok{mf <-}\StringTok{ }\KeywordTok{eval}\NormalTok{(temp, }\KeywordTok{parent.frame}\NormalTok{())}
  \NormalTok{Y <-}\StringTok{ }\KeywordTok{model.extract}\NormalTok{(mf, }\StringTok{"response"}\NormalTok{)}
  
  \NormalTok{if (!}\KeywordTok{inherits}\NormalTok{(Y, }\StringTok{"Surv"}\NormalTok{)) }
      \KeywordTok{stop}\NormalTok{(}\StringTok{"Response must be a survival object"}\NormalTok{)}
  \NormalTok{type <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(Y, }\StringTok{"type"}\NormalTok{)}
  
  \NormalTok{if (type !=}\StringTok{ "right"} \NormalTok{&&}\StringTok{ }\NormalTok{type !=}\StringTok{ "counting"}\NormalTok{) }
      \KeywordTok{stop}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"Cox model doesn't support }\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, type, }\StringTok{"}\CharTok{\textbackslash{}"}\StringTok{ survival data"}\NormalTok{, }
          \DataTypeTok{sep =} \StringTok{""}\NormalTok{))}
  \NormalTok{if (}\KeywordTok{length}\NormalTok{(beta_0) ==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
    \NormalTok{beta_0 <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(beta_0, }\KeywordTok{ncol}\NormalTok{(mf)-}\DecValTok{1}\NormalTok{)}
  \NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Początkowa zamiana kolejności obserwacji w wejściowym zbiorze.

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{if (reorderObs) \{}
    \NormalTok{obsOrder <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(data))}
    \NormalTok{mf <-}\StringTok{ }\NormalTok{mf[obsOrder, ]}
    \NormalTok{Y <-}\StringTok{ }\NormalTok{Y[obsOrder, ]}
  \NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Wprowadzenie zmiennych pomocniczych.

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{j <-}\StringTok{ }\DecValTok{0} \CommentTok{# number of an algorithm's step}
  \NormalTok{diff <-}\StringTok{ }\DecValTok{0} \CommentTok{# differences between estimates along steps}
  \NormalTok{i <-}\StringTok{ }\DecValTok{0} \CommentTok{# indicator of a batch sample}
  \NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(data)}
  \NormalTok{batchSamplesStarts <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,n, batchSize) }\CommentTok{# indexes of starts of batch samples}
  \NormalTok{epochs_n <-}\StringTok{ }\DecValTok{1}\CommentTok{# indicator of the present epochs number}
  \NormalTok{beta_j <-}\StringTok{ }\NormalTok{beta_0}
\end{Highlighting}
\end{Shaded}

gdzie
\[ \text{diff}_{j} = \parallel \beta_{j+1}-\beta_j\parallel < \epsilon .\]
Sprawdzenie warunku zbieżności algorytmu.

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{while ( j ==}\StringTok{ }\DecValTok{0} \NormalTok{|}\StringTok{ }\NormalTok{(diff <}\StringTok{ }\NormalTok{eps &}\StringTok{ }\NormalTok{epochs_n <=}\StringTok{ }\NormalTok{epoch) )\{}
    \NormalTok{j <-}\StringTok{ }\NormalTok{j}\DecValTok{+1}
    \NormalTok{i <-}\StringTok{ }\NormalTok{i}\DecValTok{+1}
\end{Highlighting}
\end{Shaded}

Rozpoczęcie algorytmu. Dla losowej kolejności obserwacji, weź pierwszą
porcję obserwacji, której wielkość ustawiona jest dzięki parametrowi
\texttt{batchSize}. Tak powstaje podzbiór obserwacji oznaczany przez
\(\mathcal{B}\), dla którego zachodzi \(|\mathcal{B}| = b\), a \(b\)
odpowiada wartości ustawionej w parametrze \texttt{batchSize}. Indeksy
obserwacji należace do zbioru \(\mathcal{B}\) zdefiniujmy jako
\(\mathcal{B}_{\text{ind}} = \{i: X_i \in \mathcal{B} \}\).

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{if (i <}\StringTok{ }\KeywordTok{length}\NormalTok{(batchSamplesStarts)-}\DecValTok{1}\NormalTok{)\{}
    \NormalTok{batchSample_variables <-}\StringTok{ }\NormalTok{mf[batchSamplesStarts[i]:(batchSamplesStarts[i]+batchSize}\DecValTok{-1}\NormalTok{), ]}
    \NormalTok{batchSample_response <-}\StringTok{ }\NormalTok{Y[batchSamplesStarts[i]:(batchSamplesStarts[i]+batchSize}\DecValTok{-1}\NormalTok{), ]}
  \NormalTok{\} else \{}
    \NormalTok{if (i ==}\StringTok{ }\KeywordTok{length}\NormalTok{(batchSamplesStarts)-}\DecValTok{1}\NormalTok{) \{}
      \CommentTok{# last batch sample can me shorter than all others}
      \NormalTok{batchSample_variables <-}\StringTok{ }\NormalTok{mf[batchSamplesStarts[i]:(n), ]}
      \NormalTok{batchSample_response <-}\StringTok{ }\NormalTok{Y[batchSamplesStarts[i]:(n), ]}
    \NormalTok{\} else \{}
      \NormalTok{i <-}\StringTok{ }\DecValTok{1}
      \NormalTok{batchSample_variables <-}\StringTok{ }\NormalTok{mf[batchSamplesStarts[i]:(batchSamplesStarts[i]+batchSize}\DecValTok{-1}\NormalTok{), ]}
      \NormalTok{batchSample_response <-}\StringTok{ }\NormalTok{Y[batchSamplesStarts[i]:(batchSamplesStarts[i]+batchSize}\DecValTok{-1}\NormalTok{), ]}
      \NormalTok{epochs_n <-}\StringTok{ }\NormalTok{epochs_n +}\StringTok{ }\DecValTok{1} \CommentTok{# epoch has passed}
      \CommentTok{# so reorder samples}
        \NormalTok{if (reorderObs) \{}
          \NormalTok{obsOrder <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(data))}
          \NormalTok{mf <-}\StringTok{ }\NormalTok{mf[obsOrder, ]}
          \NormalTok{Y <-}\StringTok{ }\NormalTok{Y[obsOrder, ]}
        \NormalTok{\}}
    \NormalTok{\}}
  \NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Dla danego podzbioru wyznacz odpowiadającą jemu część pochodnej
częściowej funkcji log-wiarogoności ze zmienionym znakiem. Ponieważ
omawiane algorytmy rozwiązuja problem minimalizacji badanej funkcji, zaś
celem estymacji w modelu Coxa jest znalezienie parametrów modelu
maksymalizujących funkcję częściowej log-wiarogodności, zatem wzięcie do
minimalizacji funkcji z przeciwnym znakiem doprowadzi do wykorzystania
metod znajdujących minimum do znalezienia maksimum. Dla \(j\)-ego kroku
algorytmu i \(k\)-tej pochodnej cząstkowej dysponuje się podzbiorem
\(\mathcal{B}\) o liczności \(b\) (parametr \texttt{batchSize}), wtedy
\[-U^\mathcal{B}_k(\beta_j)=-\sum\limits_{i \in \mathcal{B}_\text{ind}}^{}U^\mathcal{B}_{k_{i}}(\beta_j)=-\sum\limits_{i \in \mathcal{B}_\text{ind}}^{}Y_i\Big(X_{ik}-\dfrac{\sum\limits_{l\in \mathscr{R}_\mathcal{B}(t_i)}^{} X_{lk} e^{X_l'\beta_j}}{\sum\limits_{l\in \mathscr{R}_\mathcal{B}(t_i)}^{} e^{X_l'\beta_j}}\Big),\]
gdzie \(b\) oznacza wielkość podzbioru \(\mathcal{B}\), zaś
\(\mathscr{R}_\mathcal{B}(t_i)\) to zbiór ryzyka dla podzbioru
\(\mathcal{B}\) w czasie \(t_i\).

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{U_ik <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{ncol =} \KeywordTok{ncol}\NormalTok{(mf)-}\DecValTok{1}\NormalTok{,}
                 \DataTypeTok{nrow =} \KeywordTok{nrow}\NormalTok{(batchSample_variables))}
  \NormalTok{U_k <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(mf)-}\DecValTok{1}\NormalTok{)}
  \NormalTok{for ( k in }\DecValTok{2}\NormalTok{:}\KeywordTok{ncol}\NormalTok{(mf)) \{ }\CommentTok{# 1st dimension is Y}
    \NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(batchSample_variables))\{}
      \NormalTok{l <-}\StringTok{ }\KeywordTok{which}\NormalTok{(batchSample_response[, }\DecValTok{1}\NormalTok{] <=}\StringTok{ }\NormalTok{batchSample_response[i, }\DecValTok{1}\NormalTok{])}
      \NormalTok{U_ik[i,k] <-}\StringTok{ }\NormalTok{-batchSample_variables[i, k] +}\StringTok{ }\KeywordTok{sum}\NormalTok{(batchSample_variables[l, k]*}
\StringTok{                                           }\KeywordTok{exp}\NormalTok{(batchSample_variables[l, ]%*%beta_j)/}
\StringTok{            }\KeywordTok{sum}\NormalTok{(}\KeywordTok{exp}\NormalTok{(batchSample_variables[l, ]%*%beta_j)) }
            
    \ErrorTok{\}}
  \NormalTok{U_k[k] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(U_ik[, k])  }
  \ErrorTok{\}}
\end{Highlighting}
\end{Shaded}

Następnie zaktualizuj parametry modelu.
\[\beta_{k_{j+1}} = \beta_{k_{j}} - \alpha_{j}U^{\mathcal{B}}_{k}(\beta_{k_{j}})\]

\begin{Shaded}
\begin{Highlighting}[]
   \NormalTok{beta_j <-}\StringTok{ }\NormalTok{beta_j -}\StringTok{ }\KeywordTok{learningRates}\NormalTok{(j)*U_k}
\end{Highlighting}
\end{Shaded}

Przypisz nowy warunek stopu.

\begin{Shaded}
\begin{Highlighting}[]
   \NormalTok{diff <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{learningRates}\NormalTok{(j)*U_k))}
  \ErrorTok{\}}
\end{Highlighting}
\end{Shaded}

Zwrócenie parametrów funkcji, gdy spełniony chociaż jeden warunek stopu.

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{fit <-}\StringTok{ }\KeywordTok{list}\NormalTok{()}
  \NormalTok{fit$Call <-}\StringTok{ }\NormalTok{Call}
  \NormalTok{fit$mf <-}\StringTok{ }\NormalTok{mf}
  \NormalTok{fit$coeff <-}\StringTok{ }\NormalTok{beta_j}
  \NormalTok{fit$epochs_n <-}\StringTok{ }\NormalTok{epochs_n}
  \NormalTok{fit}
\ErrorTok{\}}
\CommentTok{#coxphSGD(Surv(time, status) ~ ph.ecog + age, data=lung)}
\end{Highlighting}
\end{Shaded}

