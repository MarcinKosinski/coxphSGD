\chapter*{Wprowadzenie}
\vspace{-20pt}
W ciągu ostatniej dekady rozmiary danych rosły szybciej niż prędkość procesorów. W tej sytuacji możliwości statystycznych metod uczenia maszynowego stały się ograniczone bardziej przez czas obliczeń niż przez rozmiary zbiorów danych. Rozwiązania kompromisowe w~przypadku dużej skali danych związane są ze złożonością obliczeniową zasadniczych algorytmów optymalizacyjnych, których należy dokonywać w nietrywialny sposób. Jednym z~takich rozwiązań są algorytmy optymalizacyjne oparte o stochastyczny spadek gradientu \citep{bott1, bott2, widrow2}, które wykazują dużą wydajność w trakcie pracy z danymi wielkiej skali.

W niniejszej pracy przedstawiono algorytm estymacji współczynników w modelu Coxa metodą stochastycznego spadku gradientu. Opisany algorytm może z powodzeniem być stosowany w analizach czasu do wystąpienia zdarzenia, w których liczba zmiennych znacząco przekracza liczbę obserwacji. Przygotowana metoda estymacji współczynników z wykorzystaniem algorytmu optymalizacji metodą stochastycznego spadku gradientu może być stosowana w~analizach przeżycia z dziedziny biologii molekularnej, bioinformatycznych badań przesiewowych dotyczących ekspresji genów czy w analizach opartych o mikromacierze DNA, które są szeroko stosowane w diagnostyce, leczeniu i badaniach medycznych. Zaprezentowana schemat estymacji współczynników w modelu Coxa z wykorzystaniem algorytmu optymalizacji metodą stochastycznego spadku gradientu jest podejściem nowym i nie spotkanym do tej pory w~literaturze, jest odporna na problem współliniowości zmiennych oraz sprawdza się w sytuacji ciągłej poprawy współczynników dla napływających danych (\textit{ang. streaming data}).

Analiza przeżycia jest starą, jednak wciąż aktywną dziedziną badań znajdującą nowe zastosowania w wielu dziedzinach, takich jak: biostatystyka, socjologia, ekonomia, demografia czy w~naukach inżynieryjnych \citep{heckman, collett, boxst, hosmer}. Najbardziej charakterystyczną cechą typowych danych, jakimi posługuje się w analizie przeżycia, jest obecność obiektów, w których końcowe zdarzenie nastąpiło (wówczas ma się do czynienia z obserwacjami \textit{kompletnymi}) oraz obiektów, w~których to zdarzenie (jeszcze) nie nastąpiło (obserwacja \textit{ucięta}). Ta specyficzna postać danych statystycznych doprowadziła do powstania specjalnych metod stosowanych tylko w~analizie czasu trwania zjawisk. Analiza przeżycia charakteryzuje relacje pomiędzy czasem do wystąpienia zdarzenia a zmiennymi objaśniającymi \citep{kalf, oakes} i~do niedawna ograniczona była jedynie do zastosowań oraz analiz opartych na garści zmiennych objaśniających przy maksymalnie kilku tysiącach obserwacji. Ostatnie dokonania w obszarach rozwoju technik pozyskiwania danych i ułatwiony wraz z postępem cywilizacyjnym dostęp do większej mocy obliczeniowej spowodowały wzmożenie zainteresowania danymi z potencjalnie setkami tysięcy, a nawet milionami zmiennych. Przykładem mogą być nowe technologie w genomice produkujące wielowymiarowe mikromacierze ekspresji genów, w których liczba zmiennych prognozujących przeżycie może sięgać rozmiarom rzędu $10^5$ lub nawet większym. Inne przykłady zastosowań analizy przeżycia dla danych wielkiej skali to badania monitorowania medycznych zdarzeń niepożądanych, wielopomiarowe i rozłożone w czasie badania kliniczne czy analizy eksploracyjne (\textit{ang. data mining}) danych biznesowych.

Praca przedstawia podejście do analizy przeżycia z wykorzystaniem modelu proporcjonalnych hazardów Coxa \citep{cox}. Jak podaje \cite{assel}, omawiany model jest jednym z najszerzej stosowanych modeli w onkologicznych publikacjach naukowych, ale także jedną z najmniej rozumianych metod statystycznych. Wynika to z łatwego dostępu do pakietów statystycznych zawierających programy do analizy przeżyć, modeli regresji i~analiz wielowariantowych, ale prawie nigdy nie zawierających dobrego opisu podstawowych zasad działania modelu Coxa. Dostarczają one wyłącznie instrukcje, jak wprowadzić dane i~uruchomić odpowiednie procedury w celu uzyskania wyniku. Poniższa praca zawiera pełny opis metodologii modelu proporcjonalnych hazardów Coxa, w tym wyjaśnienie najważniejszych pojęć. W odróżnieniu do modeli parametrycznych analizy czasu do wystąpienia zdarzenia \citep{klein, collett, hosmer}, model Coxa oferuje większą elastyczność, dzięki swojej semi-parametrycznej naturze, dając jednocześnie  współczynniki regresji, które są łatwo interpretowalne. Zazwyczaj współczynniki w modelu Coxa otrzymuje się dzięki maksymalizacji częściowej funkcji log-wiarogodności modelu. Jednak w~sytuacji przekleństwa wymiarowości standardowe metody estymacji metodami spadku gradientu, takimi jak metoda Cauchy'ego czy Raphsona-Newtona zawodzą z racji na zbyt złożone i długotrwałe obliczenia. Dodatkowy problem stanowi współliniowość zmiennych, co utrudnia utrzymanie numerycznej stabilności algorytmów optymalizacyjnych.

W~literaturze zaproponowano wiele rozwiązań problemu współliniowości poprzez dodanie do funkcji częściowej log-wiargodności dodatkowego parametru związanego z karą dla modelu za zbyt dużą liczbę zmiennych w trakcie estymacji \citep{parkm, sohn, goemann}. Metoda ta, zwana regularyzacją, prowadzi do zminimalizowana liczby zmiennych w modelu poprzez wyzerowanie współczynników dla nieistotnych statystycznie zmiennych. Jednak powyższe rozwiązania znalazły zastosowania jedynie dla zbiorów danych małej skali. Takie podejście nie przeniosło się na zbiory dużej skali ze względu na występowanie w~algorytmie spadku gradientu Raphsona-Newtona kosztownych kroków obliczeniowych algorytmu wymagających odwracania wielkich macierzy, co numerycznie nie jest proste. Ponadto, jak opisuje \cite{mital} możliwe obejścia i przybliżenia często prowadzą do dużych różnic współczynników, złego dopasowania modelu czy słabego oszacowania predykcyjnej dokładności. Rozwój badań nad danym problemem \citep{KIMKIM} doprowadził do powstania metod wykorzystujących jedynie pierwszą pochodną częściowej penalizowanej (z~parametrem regularyzacji) funkcji log-wiarogodności w modelu Coxa \citep{sohn, KIM, mital}. Zabiegi wykorzystujące jedynie pierwszą pochodną są odporne na problemy danych dużej skali, gdyż nie wymagają obliczania i odwracania macierzy Hesjanu. 

Podejścia te trudno wykorzystać w sytuacjach napływu danych, gdy nie dysponuje się wszystkimi obserwacjami jednocześnie, a zachodzi potrzeba wykorzystania tych już zaobserwowanych do estymacji współczynników oraz rodzi się okazja do poprawy ich estymatorów z każdą nową obserwacją \citep{bottDOD}. Dlatego w warunkach napływu danych, możliwym rozwiązaniem jest zastosowanie algorytmu optymalizacji metodą stochastycznego spadku gradientu, który w wielu modelach do estymacji współczynników wymaga jedynie jednej obserwacji. W~modelu proporcjonalnych hazardów Coxa wykorzystanie metody stochastycznego spadku gradientu możliwe jest dla zaobserwowanych kilku obserwacji, z racji na postać częściowej funkcji log-wiarogodności, której kolejne składniki są zależne od innych obserwacji. Szerokie badania w celu wykorzystania metody stochastycznego spadku gradientu do estymacji w~modelu Coxa prowadzą pracownicy \textit{Harvard Laboratory for Applied Statistical Methodology \& Data Science}, którzy zaproponowali podejście (\textit{ang.~implicit}) uwikłanego stochastycznego spadku gradientu \citep{toulis}.  

 W niniejszej pracy zaprezentowano prostsze podejście wykorzystania algorytmu stochastyczny spadek gradientu do estymacji współczynników rozważanego modelu proporcjonalnych hazardów Coxa. Wprowadzone rozwiązanie opiera się na optymalizacji częściowej funkcji log-wiarogodności konstruowanej jedynie do obecnie zaobserwowanego podzbioru obserwacji. Taka strategia pozwala na efektywne osiągnięcie zbieżności algorytmu optymalizacji w sytuacji napływających danych, o dużym rozmiarze zmiennych objaśniających, przy jednoczesnej odporności na ich współliniowość oraz umożliwia otrzymanie oszacowań współczynników po każdym zaobserwowanym podzbiorze obserwacji.

Rozdział \ref{chap1} pracy opisuje metodę największej wiarogodności i jej matematyczne podstawy, dzięki którym możliwe jest wyznaczanie estymatorów największej wiarogodności. Uzasadniona również poprawność ich wykorzystywania poprzez udowodnienie ich własności, takich jak: asymptotyczna normalność, asymptotyczna nieobciążoność oraz zgodność. 

Rozdział \ref{chap2} poświęcony jest modelowi proporcjonalnych hazardów Coxa. Przedstawione są podstawowe terminy i definicje analizy przeżycia. Rozdział opisuje niezbędne założenia modelu Coxa oraz prezentuje metodę analitycznej estymacji w oparciu o szeroko opisaną częściową funkcję log-wiarogodności modelu i estymatory największej wiarogodności. Ta część pracy kończy się opisaniem metody generowania danych w modelu Coxa oraz implementacją funkcji generującej dane do analizy przeżycia pochodzące z rozkładu Weibulla. 

W rozdziale \ref{numPAJ} scharakteryzowany jest algorytm numerycznej optymalizacji metodą stochastycznego spadku gradientu. Ukazane zostały jego wady i zalety oraz przedstawiono różnice między tym algorytmem a algorytmami spadku gradientu rzędu I (Cauchy'ego) oraz spadku gradientu rzędu II (Raphsona-Newtona). Ten fragment pracy podsumowany jest implementacją trzech omówionych algorytmów numerycznej optymalizacji dla modelu regresji logistycznej. Implementacja posłużyła do przeprowadzenia symulacji, dzięki którym możliwe było graficzne ukazanie różnic w ścieżkach zbiegania do optimum między algorytmami spadku gradientu i ich stochastycznym odpowiednikiem. 

Rozdział \ref{rozdz4} przedstawia matematyczne podstawy zastosowania algorytmu numerycznej optymalizacji metodą stochastycznego spadku gradientu do estymacji współczynników w modelu proporcjonalnych hazardów Coxa w oparciu o analityczną metodę estymacji największej wiarogodności zastosowaną do częściowej funkcji log-wiarogodności skonstruowaną jedynie dla obecnie zaobserwowanego podzbioru danych w sytuacji napływu danych. Opisane są w~nim warunki konieczne, jakie musi spełniać zaobserwowany podzbiór danych, aby nie przerwać procesu optymalizacji częściowej funkcji log-wiarogodności. Dodatkowo zaprezentowano implementację opisanej metody w pseudo kodzie oraz podano implementację wyrażoną w~języku~$\mathcal{R}$ \citep{programikr, biecek1}. Rozdział \ref{rozdz4} kończy się przeprowadzeniem symulacji mających na celu wygenerowanie danych z modelu Coxa, dzięki wykorzystaniu implementacji z rozdziału \ref{chap2}, które wykorzystano w przygotowanym algorytmie do wyestymowania współczynników w modelu Coxa. Wyniki symulacji przedstawiono na wykresach i~porównano ścieżki optymalizacji dla różnych wielkości zaobserwowanych podzbiorów obserwacji wykorzystanych do jednego kroku algorytmu. 

Praca kończy się rozdziałem \ref{chap5}, w którym metoda estymacji w modelu Coxa proporcjonalnych hazardów z wykorzystaniem stochastycznego spadku gradientu zastosowana jest do analizy na prawdziwych danych pochodzących z badania \textit{The Cancer Genome Atlas} (TCGA). Rozdział referuje genetyczne podstawy nowotworzenia oraz opisuje dane z TCGA wraz z procesem ich pozyskania i przetwarzania \citep{kosa1, kosa2, kosa3}. Analiza ma na celu sprawdzenie wpływu występowania mutacji w danym genie na czas do wystąpienia zdarzenia niepożądanego, jakim jest śmierć w~wyniku choroby nowotworowej.

Zestawienie i podsumowanie kodów pakietu $\mathcal{R}$ użytych do symulacji oraz analizy przeżycia można znaleźć w Dodatku \ref{docCoxSGD}. Implementacja estymacji w modelu Coxa proporcjonalnych hazardów metodą stochastycznego spadku gradientu oraz funkcje symulujące dane i~generujące wykresy porównujące trajektoria zbieżności algorytmu zostały opakowane w pakiet do $\mathcal{R}$ o nazwie \texttt{coxSGD} \citep{kosa0} oraz umieszczone w internecie pod adresem \url{https://github.com/MarcinKosinski/coxphSGD}. Dokumentacja pakietu w języku angielskim została przedstawiona w Dodatku \ref{docCoxSGD}.

\chapter*{Podstawy modelu statystycznego}

W pracy zakłada się znajomość podstaw statystyki matematycznej. Aby ujednolicić oznaczenia, wprowadzono klasyczną terminologię w oparciu~o~\cite{niemiro}.

\begin{definition}
\textbf{Model statystyczny} określamy przez podanie rodziny $\{ \mathbb P_{\theta}:\theta\in\Theta\} $ rozkładów prawdopodobieństwa na przestrzeni próbkowej $\Omega$ oraz zmiennej losowej $X : \Omega \rightarrow \mathcal{X}$, którą traktujemy jako obserwację. Zbiór $\mathcal{X}$ nazywamy przestrzenią obserwacji, zaś $\Theta$ nazywamy przestrzenią parametrów. \\
\end{definition}
Symbol $\theta$ jest nieznanym parametrem opisującym rozkład badanego zjawiska. Może być jednowymiarowy lub wielowymiarowy. Determinując opis zjawiska poprzez podanie parametru $\theta$, jednoznacznie wyznaczany jest rozkład rozważanego zjawiska spośród całej rodziny rozkładów prawdopodobieństwa $\{ \mathbb P_{\theta}:\theta\in\Theta\}$, co umożliwia określenie prawdziwości tezy.
\par
Zakłada się, że przestrzeń próbkowa $\Omega$ jest wyposażona w $\sigma$-ciało $\mathcal{F}$. Wtedy:
\begin{definition}
\textbf{Przestrzenią statystyczną} nazywa się trójkę $(\mathcal{X},\mathcal{F},\{\mathbb P_{\theta}:\theta\in\Theta\})$.
\end{definition}
Wprowadzenie $\sigma$-ciała $\mathcal{F}$ sprawia, że przestrzeń statystyczna staje się przestrzenią mierzalną, a więc można na niej określić rodzinę $\{ \mathbb P_{\theta}:\theta\in\Theta\} $, dzięki której da się ustalić prawdopodobieństwa zajścia wszystkich zjawisk w rozważanej teorii.

W celu budowania niezbędnych pojęć potrzebna jest również definicja losowej próby statystycznej, zazwyczaj nazywanej \textit{próbką}.

\begin{definition}
 \textbf{Losową próba statystyczną} nazywamy zbiór obserwacji statystycznych wylosowanych z populacji, które są realizacjami ciągu zmiennych losowych o rozkładzie takim jak rozkład populacji.
 \end{definition}
