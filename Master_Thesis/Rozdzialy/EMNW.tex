\chapter{Estymacja metodą największej wiarogodności}\label{chap1}
\begin{flushright}
\textit{The making of maximum likelihood was one of \\
the most important developments in 20th century statistics. \\
It was the work of one man but it was no simple process (...). \\
John Aldrich o R. A. Fisher'ze
}
\end{flushright}

\section{Estymacja}

Estymacja to dział wnioskowania statystycznego, będący zbiorem metod pozwalających na uogólnianie wyników badania próby losowej na nieznaną postać i parametry rozkładu zmiennej losowej całej populacji oraz szacowanie błędów, wynikających z tego uogólnienia.

W statystyce parametrycznej zakłada się, że rozkład prawdopodobieństwa
opisujący doświadczenie należy do rodziny $\{\mathbb{P}_{\theta} : \theta \in \Theta\}$, ale nieznany jest
parametr $\theta$. Można go jednak szacować dzięki estymatorom opartym na statystykach.

\begin{definition}
\textbf{Statystyka}, dla $X=(X_1,\dots,X_n)$, to odwzorowanie mierzalne $T: \mathcal{X} \rightarrow \mathcal{R}.$
\end{definition}

\begin{definition}
\textbf{Estymatorem} parametru $\theta$ nazywamy dowolną statystykę
$T = T(X)$, gdzie $X$ to próba z badanego rozkładu, o wartościach w zbiorze $\Theta$. 
\end{definition}

Interpretuje się $T$ jako przybliżenie $\theta$ i często estymator $\theta$ oznacza symbolem $\hat{\theta}$. Niekiedy potrzebna jest również estymacja $g(\theta)$, gdzie $g$ to ustalona funkcja.

Pewne estymatory mające odpowiednie własności są preferowane nad inne ze względu na większą precyzję bądź ufność oszacowania danego estymatora. Poniżej przedstawione są dwie ważne definicje związane z jakością estymatorów \citep{niemiro}, gdy rozmiar próbki $X_1, \dots , X_n$ jest duży. Mówi się wtedy o własnościach asymptotycznych estymatorów, które z matematycznego punktu widzenia są twierdzeniami
granicznymi, w których $n$ dąży do nieskończoności. Dzięki tym twierdzeniom możliwe jest opisanie przybliżonych zachowań estymatorów dla dostatecznie dużych próbek. Niestety, teoria asymptotyczna nie dostarcza informacji o tym, jak duża powinna być próbka, żeby przybliżenie było dostatecznie dobre.

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{nieobciążony}, \text{jeśli dla każdego $n$} 
$$\mathbb{E}\hat{g}(X_1, \dots , X_n) = g(\theta).$$

\end{definition}

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{zgodny},
jeśli dla każdego $\theta \in \Theta$
$$ \lim\limits_{n \rightarrow \infty} \mathbb{P}_{\theta}(|\hat{g}(X_1,\dots,X_n) -g(\theta)| \leq \varepsilon ) = 1,$$
dla każdego $\varepsilon > 0.$
\end{definition}

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{mocno zgodny},
jeśli %dla każdego $\theta \in \Theta$
$$\mathbb{P}_{\theta}\Big(\lim\limits_{n \rightarrow \infty}\hat{g}(X_1,\dots,X_n)=g(\theta) \Big) = 1.$$
\end{definition}


Zgodność (mocna zgodność) znaczy tyle, że
$$\hat{g}(X_1, \dots , X_n) \rightarrow g(\theta), \ \ \ \ \ (n \rightarrow \infty)$$
według prawdopodobieństwa (prawie na pewno). Interpretacja jest taka: estymator jest uznany za
zgodny, jeśli zmierza do estymowanej wielkości przy nieograniczonym powiększaniu badanej próbki.

Jednak zgodność (nawet w mocnym sensie) nie jest satysfakcjonującą własnością estymatora,
a zaledwie minimalnym żądaniem, które powinien spełniać każdy przyzwoity estymator. Dlatego od niektórych estymatorów wymaga się silniejszych właściwości, takich jak asymptotyczna normalność.

\begin{definition}
\marginnote{\small{$\overset{d}{\rightarrow}$ oznacza zbieżność wg rozkładu.}}[1cm]
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{asymptotycznie normalny}, jeśli dla każdego $\theta \in \Theta$ istnieje funkcja $\sigma^2(\theta)$, zwana asymptotyczną wariancją, taka że
$$\sqrt{n}(\hat{g}(X_1,\dots,X_n) -g(\theta))  \overset{d}{\rightarrow} \mathcal{N}(0, \sigma^2(\theta)), \ \ \ (n \rightarrow \infty).$$
\end{definition}


Oznacza to, że rozkład prawdopodobieństwa statystyki $\hat{g}(X_1,\dots,X_n)$ jest dla dużych $n$ zbliżony do rozkładu normalnego o średniej $g(\theta)$ i wariancji $\frac{\sigma^2(\theta)}{n}$ oznaczanego jako $$\mathcal{N}\Big(g(\theta), \dfrac{\sigma^2(\theta)}{n}\Big).$$ Inaczej mówiąc, estymator jest asymptotycznie normalny, gdy: 
$$\lim\limits_{n \rightarrow \infty} \mathbb{P}_{\theta} \Big(\dfrac{\sqrt{n}}{\sigma(\theta)}(\hat{g}(X_1,\dots,X_n) -g(\theta) \leq a \Big) = \Phi(a),$$
gdzie $\Phi(x)$ to dystrybuanta standardowego rozkładu normalnego $\mathcal{N}(0,1)$.

Asymptotyczna normalność mówi, że estymator nie tylko zbiega do nieznanego parametru, lecz także że zbiega wystarczająco szybko, jak $\frac{1}{\sqrt{n}}$, czyli, że 
$$\mathbb{P}_{\theta} \Big(\dfrac{\sqrt{n}}{\sigma(\theta)}(\hat{g}(X_1,\dots,X_n) -g(\theta) \leq a \Big) - \Phi(a) = f(n) \in o(\frac{1}{\sqrt{n}}) . $$ 

Każdy asymptotycznie normalny estymator jest zgodny, choć nie musi być \textit{\text{mocno zgodny}}.

W dalszej części rozdziału zostanie wprowadzone pojęcie estymatora największej wiarogodności oraz zostaną udowodnione jego właściwości, co pokaże, że metoda największej wiarogodności, przy odpowiednich założeniach, to metoda konstrukcji rozsądnych estymatorów.

\newpage 

\section{Metoda największej wiarogodności}

Metodę największej wiarogodności wprowadził \cite{fisher2}, dla której po raz pierwszy heurystyczną procedurę numeryczną zaproponował już w \cite{fisher1}. O burzliwym procesie powstawania metody, zmianach w jej uzasadnieniu, o koncepcjach które powstały w obrębie tej metody, takich jak parametr, statystyka, wiarogodność, dostateczność czy efektywność oraz o podejściach, które Fisher odrzucił tworząc podstawy pod nową teorię, można przeczytać w~obszernej pracy dokumentalnej \citet{aldrich1}. 

Pomysł Fishera, alternatywy do metody najmniejszych kwadratów \citep{legendre1, gauss1} był rozwijany i szeroko stosowany później przez wielu statystyków i wciąż znajduje obszerne zastosowania w wielu obszarach estymacji statystycznej \citep{hutch1, kenward1, millar1}.

Aby zdefiniować estymator oparty o metodę największej wiarogodności, należy najpierw wprowadzić pojęcie funkcji wiarogodności.

\begin{definition}
\textbf{Funkcją wiarogodności} nazywamy funkcję $L : \Theta \rightarrow \mathbb{R}$ daną wzorem $$ L(\theta) =L(\theta;x_1, \dots , x_n) = f(\theta; x_1, \dots , x_n),$$
którą rozważamy jako funkcję parametru $\theta$ przy ustalonych wartościach obserwacji $x_1, \dots , x_n$, gdzie $$ f(\theta; x_1, \dots , x_n) = \begin{dcases*}
 \mathbb{P}_{\theta}( X_1 = x_1, \dots , X_n = x_n), & jeśli $\mathbb{P}_{\theta}$ ma rozkład dyskretny,  \\
 f_{\theta}(x_1, \dots , x_n), & jeśli $\mathbb{P}_{\theta}$ ma rozkład absolutnie ciągły.
\end{dcases*}$$

\end{definition}

Oznacza to, że wiarogodność jest właściwie tym samym, co gęstość prawdopodobieństwa,
ale rozważana jako funkcja parametru $\theta$, przy ustalonych wartościach obserwacji
$x = X(\omega)$.


\begin{definition}
\textbf{Estymatorem największej wiarogodności} parametru $\theta$, oznaczanym ENW($\theta$), nazywamy wartość parametru, w której funkcja
wiarogodności przyjmuje supremum $$L(\hat{\theta}) = \sup_{\theta \in \Theta} L(\theta).$$

\end{definition}

Takie supremum może nie istnieć, dlatego niektóre pozycje w literaturze, w definicji estymatora największej wiarogodności, supremum zastępują wartością największą \citep{rydl1,sfu1,mit0}.

\section{Asymptotyczne własności estymatora największej wiarogodności}

W tym podrozdziale zostanie wykazane, że estymator największej wiarygodności jest
\begin{enumerate}
\item[$i$)] zgodny,
\item[$ii$)] asymptotycznie normalny,
\item[$iii$)] asymptotycznie nieobciążony.
\end{enumerate}

Asymptotyczna nieobciążoność wynika z asymptotycznej normalności. Przedstawione dowody są znane i opierają się o \cite{mit1} oraz \cite{sfu1}.



\subsubsection{Zgodność estymatora największej wiarogodności}

Chcąc wykazać zgodność estymatora największej wiarogodności \textit{przy pewnych warunkach regularności}
przydatna będzie poniższa definicja i następujący Lemat.
\begin{definition}
\textbf{Funkcja log-wiarogodności} to funkcja spełniająca równanie
$$\ell(\theta) = \log(L(\theta)),$$
gdzie przyjmuje się $\ell(\theta) = -\infty$ jeśli $L(\theta) = 0$.
\end{definition}

\begin{lemma}\label{l:pierwszy}
Gdy $\theta_0$ to maksimum funkcji wiarogodności, to dla każdego \text{$\theta \in \Theta$}
\marginnote{\tiny{$\mathbb{E}_{\theta_0}$ oznacza wartość oczekiwaną względem rozkładu $\mathbb{P}_{\theta_{0}}$}}[0.5cm]
\begin{equation*}
\mathbb{E}_{\theta_0}\ell(\theta) \leq \mathbb{E}_{\theta_0}\ell(\theta_0).
\end{equation*}
\end{lemma}

\begin{proof}
Rozważając różnicę, przy założeniu ciągłości rozkładu:
\begin{equation*}
\begin{split}
\mathbb{E}_{\theta_0}\ell(\theta) - \mathbb{E}_{\theta_0}\ell(\theta_0) & = \mathbb{E}_{\theta_0}(\ell(\theta) - \ell(\theta_0) ) = \mathbb{E}_{\theta_0}(\log f(\theta; X) - \log f(\theta_0; X)) \\
 & = \mathbb{E}_{\theta_0}\log\dfrac{f(\theta; X)}{f(\theta_0; X)},
\end{split}
\end{equation*}
i pamiętając o tym, że $\log t \leq t - 1$, można dojść do
\begin{equation*}
\begin{split}
\mathbb{E}_{\theta_0}\log\dfrac{f(\theta; X)}{f(\theta_0; X)} & \leq \mathbb{E}_{\theta_0}\Big(\dfrac{f(\theta; X)}{f(\theta_0; X)} - 1 \Big) = \int_{\mathbb{R}} \Big(\dfrac{f(\theta; x)}{f(\theta_0; x)} - 1 \Big) f(\theta_0;x) dx \\ 
& = \int_{\mathbb{R}} f(\theta;x)dx - \int_{\mathbb{R}} f(\theta_0;x)dx = 1-1 =0.
\end{split}
\end{equation*}
Obie całki równają się 1 jako, że są całkami z funkcji gęstości, zaś równość w nierówności zachodzi tylko wtedy, gdy  $\mathbb{P}_{\theta}=\mathbb{P}_{\theta_0}$.
\end{proof}

Dzięki temu wynikowi możliwe jest udowodnienie poniższego Twierdzenia (\ref{twierdzonko}).

\begin{theorem}\label{twierdzonko}
Pod pewnymi \textbf{warunkami regularności} nałożonymi na rodzinę rozkładów prawdopodobieństwa, estymator największej wiarogodności $ENW(\theta)$ jest zgodny, tzn. 
$$ENW(\theta) \rightarrow \theta \ \ \text{dla } \ \ n \rightarrow \infty.$$
\end{theorem}
\begin{proof}
\ \\
1) Z definicji w $ENW(\theta)$ przyjmowana jest wartość największa funkcji $L(\theta)$, a więc tym bardziej funkcji $ \ell(\theta) = \log L(\theta)$ oraz funkcji $$ \ell_n(\theta) = \frac{1}{n}\ell(\theta) = \frac{1}{n}\log L(\theta) = \frac{1}{n}\sum\limits_{i=1}^{n}\log f(\theta;x_i)$$ (zakładając ciągłość rozkładu i niezależność $X_1,\dots,X_n$), gdyż ekstremum jest niezmiennicze ze względu na monotoniczną transformację i liniowe przekształcenie, jakim jest dzielenie.

2) Z Lematu \ref{l:pierwszy} wynika, że $\theta_0$ maksymalizuje $\mathbb{E}_{\theta_0}\ell(\theta)$.

3) Z Prawa Wielkich Liczb, które jest spełnione gdy założy się, że $x_i$ to realizacje ciągu zmiennych losowych o skończonych wartościach oczekiwanych, wynika że $$ \ell_n(\theta) = \frac{1}{n}\sum\limits_{i=1}^{n}\log f(\theta;x_i) \rightarrow \mathbb{E}_{\theta_0}\ell(\theta),$$ co ostatecznie oznacza, że $ENW(\theta)$ jest zgodny.
\end{proof}

\newpage
\subsubsection{Asymptotyczna normalność estymatora największej wiarygodności}


Fisher w swojej karierze wprowadził wiele pożytecznych pojęć stosowanych do dziś. Jednym z nich jest Informacja Fishera, która zostanie wykorzystana w dowodzie asymptotycznej normalności estymatora największej wiarogodności.

\begin{definition}\label{inf:fish}
Niech $X$ będzie zmienną losową o gęstości $f_{\theta}$, zależnej od jednowymiarowego parametru $\theta \in \Theta \subset \mathbb{R}$. \textbf{Informacją Fishera} zawartą w obserwacji $X$ nazywa się funkcję
\begin{equation}\label{wzor:fish}
\mathcal{I}(\theta) = \mathbb{E}_{\theta}(\ell'(\theta;X))^2 = \mathbb{E}_{\theta}\Big(\frac{\partial}{\partial\theta}\log f_{\theta}(X) \Big)^2,
\end{equation}
gdzie odpowiednio
\begin{align*}
\mathcal{I}(\theta) = & \int \Big(\frac{\partial}{\partial\theta}\log f_{\theta}(x) \Big)^2 f_{\theta}(x)dx \ \text{\ \ \ \ \ dla zmiennej ciągłej;} \\
\mathcal{I}(\theta) = & \sum\limits_{x}^{ } \Big(\frac{\partial}{\partial\theta}\log f_{\theta}(x) \Big)^2 \mathbb{P}_{\theta}(X=x) \ \text{dla zmiennej dyskretnej}.
\end{align*}
\end{definition}

W dowodzie asymptotycznej normalności estymatora największej wiarogodności kluczowymi założeniami są poniższe warunki regularności. Rodzina gęstości musi być dostatecznie regularna aby pewne kroki rachunkowe w dalszych rozumowaniach były
poprawne.

\begin{definition}\label{def:regur}
\textbf{Warunki regularności.}
\end{definition}
\begin{itemize}
\item[$i)$] Informacja Fishera jest dobrze określona. Zakłada się, że $\Theta$ jest przedziałem
otwartym, istnieje pochodna $\frac{\partial}{\partial\theta}\log f_{\theta}$, całka/suma we wzorze
(\ref{wzor:fish}) jest bezwzględnie zbieżna (po obłożeniu funkcji podcałkowej modułem całka istnieje i jest skończona) i $0 < \mathcal{I}(\theta) < \infty$.
\item[$ii)$] Wszystkie gęstości $f_\theta$ mają jeden nośnik, tzn. zbiór $\{x \in X : f_\theta(x) > 0\}$ nie zależy~od~$\theta$.
\item[$iii)$] Można przenosić pochodną przed znak całki, czyli zamienić kolejność
operacji różniczkowania $\frac{\partial}{\partial\theta}$ i całkowania $\int \dots dx$.
\end{itemize}

Wprowadzając takie założenia, otrzymano przydatne właściwości~Informacji~Fishera.

\begin{proposition}\label{prop:warunki}
Jeśli spełnione są warunki regularności (Definicja \ref{def:regur}) to:
\begin{align*}
(i) \ & \ \mathbb{E}_\theta\frac{\partial}{\partial\theta}\log f_{\theta}(X) = 0, \\
(ii) \ & \ \mathcal{I}(\theta) = \mathbb{V}ar_{\theta}\Big(\frac{\partial}{\partial\theta}\log f_{\theta}(X) \Big), \\
(iii) \ & \ \mathcal{I}(\theta) = -\mathbb{E}_{\theta}\Big(\frac{\partial^2}{\partial\theta^2}\log f_\theta(X) \Big).
\end{align*}
\end{proposition}

Dowód tego stwierdzenia można znaleźć w \cite{niemiro}.

\newpage
Patrząc na postać pochodnej funkcji log-wiarogodności
$$\ell'(\theta_0;x) = (\log f(\theta_0;x))' = \dfrac{f'(\theta_0;x)}{f(\theta_0;x)},$$
można wywnioskować, że nieformalnie interpretacja Informacji Fishera jest miarą tego, jak szybko zmieni się funkcja gęstości, jeśli delikatnie zmieni się parametr $\theta$ w okolicach $\theta_0$. Biorąc kwadrat i wartość oczekiwaną, czyli uśredniając po $X$, otrzymuje się uśrednioną wersję tej miary. Jeżeli Informacja Fishera jest duża, oznacza to, że gęstość zmieni się szybko, gdyby poruszyć parametr $\theta_0$, innymi słowy - gęstość z parametrem $\theta_0$ może zostać łatwo odróżniona od gęstości z parametrami nie tak bliskimi $\theta_0$.
Stąd wiemy, że możliwa estymacja $\theta_0$ oparta o takie dane jest dobra. Z drugiej strony, jeżeli
Informacja Fishera jest mała, oznacza to, że gęstość dla $\theta_0$ jest bardzo podobna do gęstości z parametrami nie tak bliskimi do $\theta_0$, a co za tym idzie, dużo ciężej będzie odróżnić tę gęstość, czyli estymacja będzie słabsza.

Informacja Fishera i warunki regularności umożliwiają udowodnienie Twierdzenia \ref{twdtwd}, z którego wynika że im większa Informacja Fishera tym mniejsza asymptotyczna wariancja estymatora prawdziwego parametru $\theta_0$.

\begin{theorem}\label{twdtwd}
Pod pewnymi \textbf{warunkami regularności} nałożonymi na rodzinę rozkładów prawdopodobieństwa, estymator największej wiarogodności jest asymptotycznie normalny, 
$$ \sqrt{n}(ENW(\theta) - \theta_0) \overset{d}{\rightarrow} \mathcal{N}\Big(0, \dfrac{1}{\mathcal{I}(\theta_0)}\Big).$$
\end{theorem}

\begin{proof}
Ponieważ $ENW(\theta)$ maksymalizuje $\ell_n(\theta) = \frac{1}{n}\sum\limits_{i=1}^{n}\log f(\theta;X)$, to $\ell'_n(\theta)=0$. \\ Dalej, korzystając z Twierdzenia o Wartości Średniej:
$$\dfrac{g(a)-g(b)}{a-b} = g'(c) \ \text{albo} \ g(a)=g(b)+g'(c)(a-b), \ \text{dla pewnego} \ c \in [a,b],$$
gdzie $g(\theta) = \ell'_n(\theta), a = ENW(\theta), b = \theta_0$, można zapisać równość
$$ 0 = \ell'_n(ENW(\theta)) = \ell'_n(\theta_0) + \ell''_n(\theta_1)(ENW(\theta)-\theta_0), \ \text{dla} \ \theta_1 \in [ENW(\theta),\theta_0],$$ 
a z niej przejść do postaci 
\begin{equation}\label{dowod1}
\sqrt{n}(ENW(\theta)-\theta_0) = - \dfrac{\sqrt{n}\ell'_n(\theta_0)}{\ell''_n(\theta_1)}.
\end{equation}
Z Lematu (\ref{l:pierwszy}) wynika, że $\theta_0$ maksymalizuje $\mathbb{E}_{\theta_0}\ell(\theta_0)$ czyli
\begin{equation}\label{row:maksymalizator}
\mathbb{E}_{\theta_0}\ell'(\theta_0) = 0,
\end{equation}
a to można wstawić do licznika w równaniu (\ref{dowod1})
\begin{equation}\label{dowod2}
\begin{split}
\sqrt{n}\ell'_n(\theta_0) & = \sqrt{n}\Big(\frac{1}{n}\sum\limits_{i=1}^{n}\ell'(\theta_0) - 0\Big) \\
& = \sqrt{n}\Big(\frac{1}{n}\sum\limits_{i=1}^{n}\ell'(\theta_0) - \mathbb{E}_{\theta_0}\ell'(\theta_0)\Big) \rightarrow \mathcal{N}\Big(0, \mathbb{V}ar_{\theta_0}(\ell'(\theta_0))\Big),
\end{split}
\end{equation}
gdzie zbieżność wynika z Centralnego Twierdzenia Granicznego. 
\newpage
Następnie można rozważyć mianownik w równaniu (\ref{dowod1}). Dla wszystkich $\theta$ wynika
$$\ell''(\theta) = \frac{1}{n}\sum\limits_{i=1}^{n}\ell''(\theta) \rightarrow \mathbb{E}_{\theta_0}\ell''(\theta)$$
z Prawa Wielkich Liczb. \\ \ \\ Dodatkowo, ponieważ $\theta_1 \in [ENW(\theta),\theta_0]$ a $ENW(\theta)$ jest zgodny (poprzedni podrozdział), to ponieważ $ENW(\theta) \rightarrow \theta_0$, to też $\theta_1 \rightarrow \theta_0$, a wtedy
$$\ell''_n(\theta_1) \rightarrow \mathbb{E}_{\theta_0}\ell''(\theta_0) = -\mathcal{I}(\theta_0)$$
z punktu $(iii)$ ze Stwierdzenia (\ref{prop:warunki}).\\ \ \\ Wtedy prawa strona równania (\ref{dowod1}), dzięki (\ref{dowod2})
$$- \dfrac{\sqrt{n}\ell'_n(\theta_0)}{\ell''_n(\theta_1)} \overset{d}{\rightarrow} \mathcal{N}\Big(0,\dfrac{\mathbb{V}ar_{\theta_0}(\ell'(\theta_0))}{(\mathcal{I}(\theta_0))^2} \Big).$$
Ostatecznie wariancja
$$\mathbb{V}ar_{\theta_0}(\ell'(\theta_0)) = \mathbb{E}_{\theta_0}(\ell'(\theta_0))^2 - (\mathbb{E}_{\theta_0}\ell'(\theta_0))^2 = \mathcal{I}(\theta_0) - 0,$$
co wynika z definicji Informacji Fishera i wzoru (\ref{row:maksymalizator}).

\end{proof}