\chapter{Estymacja metodą największej wiarogodności}
\begin{flushright}
\textit{The making of maximum likelihood was one of \\
the most important developments in 20th century statistics. \\
It was the work of one man but it was no simple process (...). \\
John Aldrich o R. A. Fisher'ze, 1997 \cite{aldrich1}
}
\end{flushright}

\section{Estymacja}

Estymacja to dział wnioskowania statystycznego będący zbiorem metod pozwalających na uogólnianie wyników badania próby losowej na nieznaną postać i parametry rozkładu zmiennej losowej całej populacji oraz szacowanie błędów wynikających z tego uogólnienia \cite{wiki1}.

W statystyce matematycznej zakłada się, że rozkład prawdopodobieństwa
opisujący doświadczenie należy do rodziny $\{\mathbb{P}_{\theta} : \theta \in \Theta\}$, ale nie zna się
parametru $\theta$. 

\begin{definition}
\textbf{Estymatorem} parametru $\theta$ nazywamy dowolna statystykę
$T = T(X)$ o wartościach w zbiorze $\Theta$. 
\end{definition}

Interpretuje się $T$ jako przybliżenie $\theta$ i często estymator $\theta$ oznacza symbolem $\hat{\theta}$.

Pewne estymatory mające odpowiednie własności są preferowane nad inne ze względu na większą precyzję bądź ufność oszacowania danego estymatora. Poniżej przedstawione są 2 ważne definicje związane z jakością estymatorów \cite{niemiro}, gdy rozmiar próbki $X_1, \dots , X_n$ jest duży. Mówi się wtedy o własnościach asymptotycznych estymatorów, które z matematycznego punktu widzenia, są twierdzeniami
granicznymi, w których $n$ dąży do nieskończoności. Dzięki tym twierdzeniom możliwe jest opisane w przybliżeniu zachowania estymatorów dla dostatecznie dużych próbek. Niestety, teoria asymptotyczna nie dostarcza informacji o tym, jak duża powinna być próbka, żeby przybliżenie było dostatecznie dobre.

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{nieobciążony}, \text{jeśli dla każdego $n$} 
$$\mathbb{E}\hat{g}(X_1, \dots , X_n) = g(\theta).$$

\end{definition}

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{zgodny},
jeśli dla każdego $\theta \in \Theta$
$$ \lim\limits_{n \rightarrow \infty} \mathbb{P}_{\theta}(|\hat{g}(X_1,\dots,X_n) -g(\theta)| \leq \varepsilon ) = 1,$$
dla każdego $\varepsilon > 0.$
\end{definition}

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{mocno zgodny},
jeśli %dla każdego $\theta \in \Theta$
$$\mathbb{P}_{\theta}\Big(\lim\limits_{n \rightarrow \infty}\hat{g}(X_1,\dots,X_n)=g(\theta) \Big) = 1.$$
\end{definition}


Zgodność (mocna zgodność) znaczy tyle, że
$$\hat{g}(X_1, \dots , X_n) \rightarrow g(\theta), \ \ \ \ \ (n \rightarrow \infty)$$
według prawdopodobieństwa (prawie na pewno). Interpretacja jest taka: estymator jest uznany za
zgodny, jeśli zmierza do estymowanej wielkości przy nieograniczonym powiększaniu badanej próbki.

Jednak zgodność (nawet w mocnym sensie) nie jest specjalnie satysfakcjonującą własnością estymatora,
a zaledwie minimalnym żądaniem, które powinien spełniać każdy przyzwoity estymator. Dlatego od niektórych estymatorów żąda się silniejszych właściwości, takich jak asymptotyczna normalność.

\begin{definition}
Estymator $\hat{g}(X_1, \dots , X_n)$ wielkości $g(\theta)$ jest \textbf{asymptotycznie normalny}, jeśli dla każdego $\theta \in \Theta$ istnieje funkcja $\sigma^2$, zwana asymptotyczną wariancją, taka że
$$\sqrt{n}(\hat{g}(X_1,\dots,X_n) -g(\theta))  \underset{D}{\rightarrow} \mathcal{N}(0, \sigma^2(\theta)), \ \ \ (n \rightarrow) \infty.$$
\end{definition}

Oznacza to, że rozkład prawdopodobieństwa statystyki $\hat{g}(X_1,\dots,X_n)$ jest dla dużych $n$ zbliżony do rozkładu $$\mathcal{N}\Big(g(\theta), \dfrac{\sigma^2(\theta)}{n}\Big).$$ Inaczej mówiąc, estymator jest asymptotycznie normalny, gdy: 
$$\lim\limits_{n \rightarrow \infty} \mathbb{P}_{\theta} \Big(\dfrac{\sqrt{n}}{\sigma(\theta)}(\hat{g}(X_1,\dots,X_n) -g(\theta) \leq a \Big) = \Phi(a).$$
Asymptotyczna normalność mówi, że estymator nie tylko zbiega do nieznanego parametru, ale również że zbiega wystarczająco szybko, jak $\dfrac{1}{\sqrt{n}}$. 

Jeśli estymator jest asymptotycznie normalny, to jest zgodny, choć nie musi
być \textit{\text{mocno zgodny}}.

W dalszej części tego rozdziału zostanie wprowadzone pojęcie estymatora największej wiarogodności oraz zostaną udowodnione dla niego jego właściwości, co utwierdzi w przekonaniu, że metoda największej wiarogodności, przy odpowiednich założeniach, jest metodą konstrukcji rozsądnych estymatorów.

\newpage 

\section{Metoda największej wiarogodności}

Metodę największej wiarogodności wprowadził R. A. Fisher w 1922 r. \cite{fisher2}, dla której po raz pierwszy procedurę numeryczną zaproponował już w 1912 r. \cite{fisher1}. O burzliwym procesie powstawania metody, o zmianach w jej uzasadnieniu, o koncepcjach, które powstały w obrębie tej metody takich jak parametr, statystyka, wiarogodność, dostateczność czy efektywność oraz o podejściach, które Fisher odrzucił tworząc podstawy pod nową teorię można przeczytać w~obszernej pracy dokumentalnej \cite{aldrich1}. 

Metoda ta, jako alternatywa dla metody najmniejszych kwadratów \cite{legendre1}, \cite{gauss1}, była rozwijana i szeroko stosowana później przez wielu statystyków i wciąż znajduje obszerne zastosowania w wielu obszarach estymacji statystycznej, np. \cite{hutch1}, \cite{kenward1}, \cite{millar1}.

Aby zdefiniować estymator oparty o metodę największej wiarogodności, należy najpierw wprowadzić pojęcie funkcji wiarogodności.

\begin{definition}
\textbf{Funkcją wiarogodności} nazywamy funkcję $L : \Theta \rightarrow \mathbb{R}$ daną wzorem $$ L(\theta;x_1, \dots , x_n) = f(\theta; x_1, \dots , x_n),$$
którą rozważamy jako funkcję parametru $\theta$ przy ustalonych wartościach obserwacji $x_1, \dots , x_n$, gdzie $$ f(\theta; x_1, \dots , x_n) = \begin{dcases*}
 \mathbb{P}_{\theta}( X_1 = x_1, \dots , X_n = x_n), & dla rozkładów dyskretnych,  \\
 f_{\theta}(x_1, \dots , x_n), & dla rozkładów absolutnie ciągłych.
\end{dcases*}$$

\end{definition}

Oznacza to, że wiarogodność jest właściwie tym samym, co gęstość prawdopodobieństwa,
ale rozważana jako funkcja parametru $\theta$, przy ustalonych wartościach obserwacji
$x = X(\omega)$.


\begin{definition}
\textbf{Estymatorem największej wiarogodności} parametru $\theta$, oznaczanym ENW($\theta$), nazywamy wartość parametru, w której funkcja
wiarogodności przyjmuje supremum $$L(\hat{\theta}) = \sup_{\theta \in \Theta} L(\theta).$$

\end{definition}

Niektóre pozycje w literaturze, w definicji estymatora największej wiarogodności, supremum zastępują wartością największą \cite{rydl1} str 14, \cite{sfu1} str 1 bądź \cite{mit0} str 11.

\section{Asymptotyczne własności estymatora największej wiarogodności}

W tym podrozdziale zostanie wykazane, że estymator największej wiarygodności jest
\begin{enumerate}
\item asymptotycznie nieobciążony, (\textcolor{red}{na pewno?})
\item zgodny,
\item asymptotycznie normalny.
\end{enumerate}

Dowody w tym rozdziale są znane w literaturze i opierają się o \cite{mit1} i \cite{sfu1}.



\subsection{Zgodność estymatora największej wiarogodności}

Chcąc wykazać zgodność estymatora największej wiarogodności (\textcolor{red}{przy pewnych warunkach regularności?})
przydatna będzie poniższa definicja i następujący Lemat.
\begin{definition}
\textbf{Funkcja log-wiarogodności} to funkcja spełniająca równanie
$$l(\theta) = \log(L(\theta)).$$
\end{definition}

\begin{lemma}\label{l:pierwszy}
Gdy $\theta_0$ to prawdziwe maksimum funkcji wiarogodności, to dla każdego \text{$\theta \in \Theta$}
$$\mathbb{E}_{\theta_0}l(\theta) \leq \mathbb{E}_{\theta_0}l(\theta_0).$$
\end{lemma}

\begin{proof}
Rozważając różnicę:
\begin{equation*}
\begin{split}
\mathbb{E}_{\theta_0}l(\theta) - \mathbb{E}_{\theta_0}l(\theta_0) & = \mathbb{E}_{\theta_0}(l(\theta) - l(\theta_0) ) = \mathbb{E}_{\theta_0}(\log f(\theta; X) - \log f(\theta_0; X)) \\
 & = \mathbb{E}_{\theta_0}\log\dfrac{f(\theta, X)}{f(\theta_0, X)},
\end{split}
\end{equation*}
i pamiętając o tym, że $\log t \leq t - 1$, można dojść do
\begin{equation*}
\begin{split}
\mathbb{E}_{\theta_0}\log\dfrac{f(\theta; X)}{f(\theta_0; X)} & \leq \mathbb{E}_{\theta_0}\Big(\dfrac{f(\theta; X)}{f(\theta_0; X)} - 1 \Big) = \int \Big(\dfrac{f(\theta; x)}{f(\theta_0; x)} - 1 \Big) f(\theta_0;x) dx \\ 
& = \int f(\theta;x)dx - \int f(\theta_0;x)dx = 1-1 =0.
\end{split}
\end{equation*}
Obie całki równają się 1 jako, że są całkami z funkcji gęstości, zaś równość w nierówności zachodzi tylko wtedy, gdy  $\mathbb{P}_{\theta}=\mathbb{P}_{\theta_0}$.
\end{proof}

Dzięki temu wynikowi możliwe jest udowodnienie poniższego Twierdzenia.

\begin{theorem}
Pod pewnymi \textbf{warunkami regularności} nałożonymi na rodzinę rozkładów prawdopodobieństwa, estymator największej wiarogodności $ENW(\theta)$ jest zgodny, tzn. 
$$ENW(\theta) \rightarrow \theta \ \ \text{dla } \ \ n \rightarrow \infty.$$
\end{theorem}
\begin{proof}
\ \\
1) Z definicji w $ENW(\theta)$ przyjmowana jest wartość największa funkcji $L(\theta)$, a więc tym bardziej funkcji $ l(\theta) = \log L(\theta)$ oraz funkcji $ l_n(\theta) = \frac{1}{n}l(\theta) = \frac{1}{n}\log L(\theta) = \frac{1}{n}\sum\limits_{i=1}^{n}\log f(\theta;X_i)$, gdyż ekstremum jest niezmiennicze ze względu na monotoniczną transformację i liniowe przekształcenie jakim jest podzielenie przez $n$.

2) Z Lematu \ref{l:pierwszy} wynika, że $\theta_0$ maksymalizuje $\mathbb{E}_{\theta_0}l(\theta)$.

3) Z Prawa Wielkich Liczb wynika, że $ l_n(\theta) = \frac{1}{n}\sum\limits_{i=1}^{n}\log f(\theta;X_i) \rightarrow \mathbb{E}_{\theta_0}l(\theta)$, co ostatecznie oznacza, że $ENW(\theta)$ jest zgodny.
\end{proof}

\newpage
\subsection{Asymptotyczna normalność estymatora największej wiarygodności}